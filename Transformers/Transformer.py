import torch
from torch.nn.modules.transformer import Transformer
from math import sqrt, log

class T5PositionEmbedding(torch.nn.Module):
    def __init__(self, num_head, num_buckets=32, max_distance=128):
        super().__init__()
        self.num_head = num_head
        self.num_buckets = num_buckets
        self.max_distance = max_distance
        # [bucket, num_head] : dim è®¾ç½®ä¸º num_head åŸå› æ˜¯è¿™é‡Œä½œä¸ºæ¯ä¸ªå¤´çš„åæ‰§ bias, ä¹Ÿç®—ä¸€ç§å¤šå¤´ä½ç½®ç¼–ç 
        self.embedding = torch.nn.Embedding(self.num_buckets, self.num_head)    # [bucket, H]

    @staticmethod
    def _relative_position_bucket(relative_position, num_buckets=32, max_distance=128):
        """
        è¾“å…¥: relative_position (Tensor)
        è¾“å‡º: bucket_ids (Tensor)
        """
        # embedding ä¸æ¥å—è´Ÿæ•°ï¼Œæ‰€ä»¥è¿™é‡Œè¦è€ƒè™‘ offset çš„é—®é¢˜    
        assert num_buckets % 4 == 0
        length = num_buckets // 2     # ä¸€ä¸ªåŒºé—´çš„é•¿åº¦
        log_len = length // 2        # ä½¿ç”¨ log çš„é•¿åº¦
        abs_relative_position = torch.abs(relative_position)
        bucket_ids = torch.where(
            abs_relative_position <= log_len,
            abs_relative_position, 
            torch.where(
                abs_relative_position < max_distance,
                log_len + torch.round(torch.log(abs_relative_position - log_len)),
                length - 1
            )
        )
        bucket_ids = bucket_ids + torch.where(relative_position > 0, length, 0)
        return bucket_ids

    @staticmethod
    def _t5_relative_position_bucket(relative_position, num_buckets=32, max_distance=128):
        """æ›´åŠ å¹³æ»‘ã€å‡åŒ€çš„æ¡¶ç”Ÿæˆæ–¹å¼"""
        num_buckets //= 2 # å•å‘æ¡¶æ•°ï¼Œä¾‹å¦‚ 16
        res = 0
        n = -relative_position # T5 ä¹ æƒ¯ï¼šè®¡ç®—ç›®æ ‡ç›¸å¯¹äºå½“å‰çš„åç§»
        
        # 2. å¤„ç†æ­£è´ŸåŠåŒº (æœªæ¥/è¿‡å»)
        res += (n < 0).to(torch.long) * num_buckets
        n = torch.abs(n)

        max_exact = num_buckets // 2 # ç²¾ç¡®åŒºè¾¹ç•Œï¼Œä¾‹å¦‚ 8
        is_small = n < max_exact
        val_if_large = max_exact + (
            torch.log(n.float() / max_exact) / 
            log(max_distance / max_exact) * (num_buckets - max_exact)
        ).to(torch.long)

        val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, num_buckets - 1))
        res += torch.where(is_small, n, val_if_large)
        return res
    
    def forward(self, seq_len):
        """ç”Ÿæˆè¿™ä¸ª seq å¯¹åº”çš„ relative id çŸ©é˜µ"""
        seq = torch.arange(0, seq_len, device=self.embedding.weight.device) # ä¿æŒè®¾å¤‡ä¸€è‡´
        relative_id = seq[:, None] - seq[None, :]   # [L, L]
        relative_bucket_id = self._relative_position_bucket(relative_id, self.num_buckets, self.max_distance).long()    # [L, L]
        position_bias = self.embedding(relative_bucket_id)  # [L, L, H]
        position_bias = position_bias.permute(2, 0, 1).unsqueeze(0)
        return position_bias

class RMS_Norm(torch.nn.Module):
    def __init__(self, hidden_dim, eps=1e-6):
        super().__init__()
        self.gamma = torch.nn.Parameter(torch.ones(hidden_dim))
        self.eps = eps

    def forward(self, x):
        # x : [B, L, D]
        x_mean = torch.mean(x ** 2, dim=-1, keepdim=True)
        return x / torch.sqrt(x_mean + self.eps) * self.gamma

class FFN(torch.nn.Module):
    def __init__(self, in_dim, hidden_dim, dropout_rate=0.1):
        super().__init__()
        self.in_dim = in_dim
        self.hidden_dim = hidden_dim

        self.Linear1 = torch.nn.Linear(self.in_dim, self.hidden_dim, bias=False)
        self.Linear2 = torch.nn.Linear(self.hidden_dim, self.in_dim, bias=False)
        self.act = torch.nn.ReLU()

    def forward(self, x):
        return self.Linear2(self.act(self.Linear1(x)))
    
class Self_Attention(torch.nn.Module):
    def __init__(self, in_dim, out_dim, num_head=8):
        super().__init__()
        assert out_dim % num_head == 0
        self.head_dim = out_dim // num_head  # å•ä¸ªå¤´è¾“å‡ºç»´åº¦
        self.num_head = num_head
        self.in_dim = in_dim
        self.out_dim = out_dim
        
        self.q = torch.nn.Linear(in_dim, out_dim, bias=False)
        self.k = torch.nn.Linear(in_dim, out_dim, bias=False)
        self.v = torch.nn.Linear(in_dim, out_dim, bias=False)
        self.o = torch.nn.Linear(out_dim, out_dim, bias=False)
    
    def forward(self, x, position_embedding=None):
        B, L, _  = x.shape
        q, k, v = self.q(x), self.k(x), self.v(x)
        # split and reshape
        # shape : [B, H, L, D]
        q = q.reshape(B, L, self.num_head, self.head_dim).permute(0, 2, 1, 3)
        k = k.reshape(B, L, self.num_head, self.head_dim).permute(0, 2, 1, 3)
        v = v.reshape(B, L, self.num_head, self.head_dim).permute(0, 2, 1, 3)

        # attention
        # score : [B, H, L, L]
        attn_score = torch.matmul(q, k.permute(0, 1, 3, 2)) / sqrt(self.head_dim)
        if position_embedding is not None:
            attn_score = attn_score + position_embedding
        score = torch.softmax(attn_score, dim=-1).matmul(v)

        # concat
        score_cat = score.permute(0, 2, 1, 3).reshape(B, L, self.out_dim)   # [B, L, D]

        # o_proj
        score_proj = self.o(score_cat)

        return score_proj
    
class Encode_Layer(torch.nn.Module):
    def __init__(self, model_dim, num_head, ffn_dim, dropout_rate=0.1):
        super().__init__()
        self.model_dim = model_dim      # Attn çš„ in_dim å’Œ out_dim æ˜¯ä¸€æ ·çš„
        self.num_head = num_head
        self.ffn_dim = ffn_dim
        self.dropout_rate = dropout_rate

        self.dropout = torch.nn.Dropout(p=self.dropout_rate)
        self.attn_norm = RMS_Norm(self.model_dim)
        self.self_attn = Self_Attention(self.model_dim, self.model_dim, self.num_head)
        self.mlp_norm = RMS_Norm(self.model_dim)
        self.mlp = FFN(self.model_dim, self.ffn_dim, self.dropout_rate)

    def forward(self, x, position_embed=None):
        attn_norm_x = self.attn_norm(x)
        attn_x = self.dropout(self.self_attn(attn_norm_x, position_embedding=position_embed)) + x
        mlp_norm_x = self.mlp_norm(attn_x)
        mlp_x = self.mlp(mlp_norm_x) + attn_x
        return mlp_x

class Encoder(torch.nn.Module):
    def __init__(
            self, num_layers, vocab_size, model_dim, num_head, ffn_dim, dropout_rate = 0.1
        ):
        super().__init__()
        self.num_layers = num_layers
        self.vocab_size = vocab_size
        self.model_dim = model_dim
        self.num_head = num_head
        self.ffn_dim = ffn_dim
        self.dropout_rate = dropout_rate

        self.embedding = torch.nn.Embedding(vocab_size, self.model_dim)
        self.encode_layers = torch.nn.ModuleList([
            Encode_Layer(model_dim, num_head, ffn_dim, dropout_rate) 
            for _ in range(num_layers)
        ])
        self.norm = RMS_Norm(self.model_dim)
        self.position_embed = T5PositionEmbedding(self.num_head, 32, max_distance=int(2 ** 10))

    def forward(self, input_ids):
        """input_ids : [B, L]"""
        position_embedding = self.position_embed(input_ids.size(1))
        x = self.embedding(input_ids)
        for layer in self.encode_layers:
            x = layer(x, position_embedding)
        x = self.norm(x)
        return x

if __name__ == "__main__":
    torch.manual_seed(42)
    
    # 1. æ¨¡æ‹Ÿè¶…å‚æ•°
    VOCAB_SIZE = 100
    D_MODEL = 32
    N_LAYERS = 3
    N_HEADS = 4
    D_FF = 128
    BATCH = 2
    SEQ_LEN = 8

    print(f"--- å¼€å§‹æµ‹è¯• Encoder (Layers={N_LAYERS}) ---")
    encoder = Encoder(N_LAYERS, VOCAB_SIZE, D_MODEL, N_HEADS, D_FF)

    # 2. æ¨¡æ‹Ÿè¾“å…¥æ•°æ® [B, L]
    input_ids = torch.randint(0, VOCAB_SIZE, (BATCH, SEQ_LEN))
    
    # 3. å‰å‘ä¼ æ’­
    try:
        output = encoder(input_ids)
        print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸï¼è¾“å‡ºå½¢çŠ¶: {output.shape}") # æœŸæœ› [2, 8, 32]
        
        # éªŒè¯è¾“å‡ºæ˜¯å¦æœ‰ NaN (æ£€æŸ¥ Norm å’Œ Log ç¨³å®šæ€§)
        if torch.isnan(output).any():
            print("âŒ è­¦å‘Šï¼šè¾“å‡ºåŒ…å« NaNï¼")
        else:
            print("âœ… æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥é€šè¿‡ (æ—  NaN)")
            
    except Exception as e:
        print(f"âŒ è¿è¡Œå´©æºƒ: {e}")

    # 4. ã€å…³é”®æµ‹è¯•ã€‘æµ…æ‹·è´éªŒè¯
    # æ£€æŸ¥ç¬¬ä¸€å±‚å’Œç¬¬äºŒå±‚çš„æƒé‡å†…å­˜åœ°å€æ˜¯å¦ç›¸åŒ
    layer0_ptr = id(encoder.encode_layers[0].self_attn.q.weight)
    layer1_ptr = id(encoder.encode_layers[1].self_attn.q.weight)
    if layer0_ptr == layer1_ptr:
        print("âŒ ä¸¥é‡é”™è¯¯ï¼šæ£€æµ‹åˆ°å±‚ä¹‹é—´å…±äº«æƒé‡ï¼ˆæµ…æ‹·è´ Bugï¼‰ï¼")
    else:
        print("âœ… å±‚ç‹¬ç«‹æ€§æ£€æŸ¥é€šè¿‡ï¼ˆå„å±‚å‚æ•°ç‹¬ç«‹ï¼‰")

    # 5. æ¢¯åº¦å›ä¼ æµ‹è¯• (éªŒè¯è®¡ç®—å›¾æ˜¯å¦é—­ç¯)
    print("\n--- æ¢¯åº¦å›ä¼ æµ‹è¯• ---")
    loss = output.mean()
    loss.backward()
    
    # æ£€æŸ¥ Embedding å±‚æ˜¯å¦æœ‰æ¢¯åº¦
    if encoder.embedding.weight.grad is not None:
        print("âœ… æ¢¯åº¦æˆåŠŸå›ä¼ è‡³ Embedding å±‚")
    else:
        print("âŒ é”™è¯¯ï¼šæ¢¯åº¦ä¸¢å¤±ï¼Œè¯·æ£€æŸ¥æ®‹å·®è¿æ¥æˆ– forward é€»è¾‘")

    print("\nğŸ‰ Encoder é˜¶æ®µæ€§æµ‹è¯•å®Œæˆï¼")