import torch
from Llama import LlamaGPT
from kvcache import LlamaLayerCache
import time

def run_comprehensive_test():
    # 1. æ¨¡æ‹Ÿå·¥ä¸šçº§é…ç½®
    class MockConfig:
        vocab_size = 32000
        dim = 256            # ç¨å¾®å¢å¤§ç»´åº¦ä»¥æš´éœ²æ½œåœ¨çš„å¯¹é½é—®é¢˜
        num_heads = 8
        num_kv_heads = 2     # å…¸å‹çš„ GQA é…ç½® (4:1)
        num_group = 8 // 2
        intermediate_size = 688
        num_layers = 4       # å¢åŠ å±‚æ•°ä»¥æµ‹è¯•æ¢¯åº¦/æ•°å€¼ç´¯ç§¯
        max_new_tokens = 1024
        rope_base = 10000.0
        pad_token_id = 0
        eos_token_id = 1

    config = MockConfig()
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"--- æ­£åœ¨ä½¿ç”¨è®¾å¤‡: {device} ---")
    
    model = LlamaGPT(config).to(device).eval()
    
    # ---------------------------------------------------------
    # æµ‹è¯•ä¸€ï¼šç»´åº¦ä¸è¿ç»­æ€§æ£€æŸ¥ (Shape & Contiguity)
    # ---------------------------------------------------------
    bsz, seq_len = 2, 32
    input_ids = torch.randint(0, config.vocab_size, (bsz, seq_len), device=device)
    mask = torch.triu(torch.full((seq_len, seq_len), float("-inf"), device=device), diagonal=1)
    
    with torch.no_grad():
        logits = model(input_ids, start_pos=0, mask=mask)
        
    assert logits.shape == (bsz, seq_len, config.vocab_size), "Logits ç»´åº¦é”™è¯¯"
    print("âœ… [æµ‹è¯• 1/6] åŸºç¡€ç»´åº¦æ£€æŸ¥é€šè¿‡")

    # ---------------------------------------------------------
    # æµ‹è¯•äºŒï¼šå› æœé®è”½ä¸¥è°¨æ€§ (Causal Mask Invariance)
    # ---------------------------------------------------------
    # ä¿®æ”¹è¾“å…¥åºåˆ—æœ«å°¾çš„è¯ï¼Œä¸åº”å½±å“åºåˆ—å¼€å¤´è¯çš„ Logits
    input_1 = torch.tensor([[1, 2, 3, 4, 5]], device=device)
    input_2 = torch.tensor([[1, 2, 3, 9, 9]], device=device) # ä¿®æ”¹åä¸¤ä¸ªè¯
    
    with torch.no_grad():
        out1 = model(input_1, start_pos=0, mask=mask[:5, :5])
        out2 = model(input_2, start_pos=0, mask=mask[:5, :5])
    
    # æ¯”è¾ƒå‰ 3 ä¸ªä½ç½®çš„è¾“å‡º
    diff_mask = torch.abs(out1[:, :3, :] - out2[:, :3, :]).max()
    assert diff_mask < 1e-5, f"å› æœé®è”½å¤±è´¥ï¼æœ€å¤§å·®å¼‚: {diff_mask.item()}"
    print("âœ… [æµ‹è¯• 2/6] å› æœé®è”½ (Causal Mask) éªŒè¯é€šè¿‡")

    # ---------------------------------------------------------
    # æµ‹è¯•ä¸‰ï¼šKV Cache ç­‰æ•ˆæ€§æµ‹è¯• (Crucial for Infra)
    # ---------------------------------------------------------
    # è¿™æ˜¯æµ‹è¯•æ¨ç†å¼•æ“æœ€å…³é”®çš„ä¸€æ­¥ï¼š
    # éªŒè¯â€œå…¨é‡ Prefillâ€å’Œâ€œé€æ­¥ Decodeâ€å¾—åˆ°çš„è¾“å‡ºæ˜¯å¦æ•°å€¼ä¸€è‡´
    input_ids = torch.tensor([[10, 20, 30, 40]], device=device)
    
    # A. å…¨é‡å‰å‘ä¼ æ’­
    with torch.no_grad():
        full_logits = model(input_ids, start_pos=0, mask=mask[:4, :4])
        target_last_logits = full_logits[:, -1, :]

    # B. æ¨¡æ‹Ÿé€æ­¥æ¨ç†
    kv_caches = [LlamaLayerCache(config, 1, device) for _ in range(config.num_layers)]
    step_logits = None
    with torch.no_grad():
        for i in range(4):
            # æ¨¡æ‹Ÿæ¯æ¬¡è¾“å…¥ä¸€ä¸ª token
            cur_input = input_ids[:, i:i+1]
            step_logits = model(cur_input, start_pos=i, kv_caches=kv_caches)
            
    # æ¯”è¾ƒæœ€åä¸€æ­¥çš„è¾“å‡º
    diff_cache = torch.abs(target_last_logits - step_logits[:, -1, :]).max()
    assert diff_cache < 1e-4, f"KV Cache æ•°å€¼ä¸ä¸€è‡´ï¼å·®å¼‚: {diff_cache.item()}"
    print("âœ… [æµ‹è¯• 3/6] KV Cache ä¸€è‡´æ€§ (Prefill vs Decode) éªŒè¯é€šè¿‡")

    # ---------------------------------------------------------
    # æµ‹è¯•å››ï¼šRoPE ç›¸å¯¹ä½ç½®å¹³ç§»éªŒè¯ (RoPE Invariance)
    # ---------------------------------------------------------
    # åŒæ ·çš„è¯åœ¨ä½ç½® 1 å’Œä½ç½® 2 äº§ç”Ÿçš„ç‰¹å¾åº”è¯¥æ˜¯ä¸åŒçš„
    token_a = torch.tensor([[100]], device=device)
    token_b = torch.tensor([[200]], device=device)
    
    with torch.no_grad():
        # æ¨¡æ‹Ÿä¸¤ç§æƒ…å†µçš„ KV Cache
        # æƒ…å†µ 1: A(pos=1) çœ‹ç€ B(pos=0)
        cache1 = [LlamaLayerCache(config, 1, device) for _ in range(config.num_layers)]
        _ = model(token_b, start_pos=0, kv_caches=cache1) # å…ˆå­˜å…¥ B
        logits_1 = model(token_a, start_pos=1, kv_caches=cache1)[:, -1, :]
        
        # æƒ…å†µ 2: A(pos=2) çœ‹ç€ B(pos=0) -> ç›¸å¯¹è·ç¦»å˜äº† (1->2)
        cache2 = [LlamaLayerCache(config, 1, device) for _ in range(config.num_layers)]
        _ = model(token_b, start_pos=0, kv_caches=cache2) # å…ˆå­˜å…¥ B
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬è·³è¿‡ä½ç½® 1ï¼Œç›´æ¥æŠŠ A æ”¾åœ¨ä½ç½® 2
        logits_2 = model(token_a, start_pos=2, kv_caches=cache2)[:, -1, :]
    
    diff_rope = torch.abs(logits_1 - logits_2).max()
    print(f"âœ… [æµ‹è¯• 4/6] ç›¸å¯¹ä½ç½®æ•æ„Ÿæµ‹è¯• (RoPE Sensitivity): {'é€šè¿‡' if diff_rope > 1e-3 else 'å¤±è´¥'}")
    assert diff_rope > 1e-3, "RoPE æ— æ•ˆï¼šæ”¹å˜ç›¸å¯¹è·ç¦»åè¾“å‡ºç«Ÿç„¶æ²¡æœ‰å˜åŒ–"

    # ---------------------------------------------------------
    # æµ‹è¯•äº”ï¼šæ•°å€¼ç¨³å®šæ€§ä¸ GQA å‹åŠ›æµ‹è¯•
    # ---------------------------------------------------------
    # æ¨¡æ‹Ÿæç«¯é•¿åºåˆ—
    long_len = config.max_new_tokens
    long_input = torch.randint(0, config.vocab_size, (1, long_len), device=device)
    try:
        with torch.no_grad():
            _ = model(long_input, start_pos=0, mask=None) # è¿™é‡Œä¸å¸¦ mask æ¨¡æ‹Ÿä¸é™é•¿çš„æ¨ç†
        print(f"âœ… [æµ‹è¯• 5/6] æ•°å€¼ç¨³å®šæ€§é€šè¿‡ (Sequence Length={long_len})")
    except RuntimeError as e:
        print(f"âŒ [æµ‹è¯• 5/6] æ˜¾å­˜ä¸è¶³æˆ–è®¡ç®—é”™è¯¯: {e}")

    # ---------------------------------------------------------
    # æµ‹è¯•å…­ï¼šç«¯åˆ°ç«¯ç”Ÿæˆç”ŸæˆåŠŸèƒ½ (Generate Logic)
    # ---------------------------------------------------------
    prompt = torch.tensor([[1, 5, 10]], device=device)
    start_time = time.time()
    generated = model.generate(prompt, max_gen_len=20)
    end_time = time.time()
    
    assert generated.shape[1] > 3, "ç”Ÿæˆé•¿åº¦å¼‚å¸¸"
    print(f"âœ… [æµ‹è¯• 6/6] ç«¯åˆ°ç«¯ç”Ÿæˆæµ‹è¯•é€šè¿‡ (ç”Ÿæˆé€Ÿåº¦: {(generated.shape[1]-3)/(end_time-start_time):.2f} tokens/s)")
    print("\nğŸš€ æ‰€æœ‰ä¸¥è°¨æ€§æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼è¯¥æ¨¡å‹å·²å…·å¤‡å·¥ä¸šçº§æ¨ç†é›å½¢ã€‚")

if __name__ == "__main__":
    run_comprehensive_test()