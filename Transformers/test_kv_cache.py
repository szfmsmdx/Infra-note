import time
import torch
from Transformer_kvcache import Model
from Config import T5Config
from Tokenizer.BPE import BPE_Tokenizer

def run_stress_test(model, src_ids, use_cache, max_tokens=256):
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()
        torch.cuda.synchronize()
    
    start_time = time.time()
    
    with torch.no_grad():
        output = model.generate(src_ids, use_cache=use_cache, max_new_token=max_tokens)
    
    if torch.cuda.is_available():
        torch.cuda.synchronize()
        peak_allocated = torch.cuda.max_memory_allocated() / (1024 ** 2)
        peak_reserved = torch.cuda.max_memory_reserved() / (1024 ** 2)
    else:
        peak_allocated, peak_reserved = 0, 0
        
    total_time = time.time() - start_time
    # æ’é™¤èµ·å§‹ token çš„æ•°é‡
    num_generated = output.size(1) - 1
    ms_per_token = (total_time / num_generated) * 1000
    
    return total_time, ms_per_token, peak_allocated, peak_reserved

if __name__ == "__main__":
    # é…ç½®æ›´æ·±ã€æ›´é•¿çš„æ¨¡å‹ä»¥è§‚å¯Ÿå·®å¼‚
    torch.manual_seed(42)
    tokenizer = BPE_Tokenizer.load("/data3/szf/Infra-note/Transformers/Tokenizer/tokenizer.pt")
    config = T5Config(tokenizer)
    config.num_layers = 12 
    config.model_dim = 768
    config.num_head = 12
    config.ffn_dim = 3072
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    base_mem = 0
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        before_model = torch.cuda.memory_allocated()
        model = Model(config).to(device).eval()
        base_mem = (torch.cuda.memory_allocated() - before_model) / (1024**2)
        print(f"ğŸ“¦ æ¨¡å‹å‚æ•°å ç”¨æ˜¾å­˜: {base_mem:.2f} MB")
    else:
        model = Model(config).eval()

    # æ¨¡æ‹Ÿé«˜å‹è¾“å…¥ï¼šBatch Size = 4, è¾“å…¥é•¿åº¦ 256, ç”Ÿæˆé•¿åº¦ 512
    batch_size = 4
    input_len = 256
    gen_len = 1024
    src_ids = torch.randint(10, 4000, (batch_size, input_len)).to(device)
    
    print(f"\nğŸš€ å¼€å§‹é«˜å‹æµ‹è¯• [Batch: {batch_size}, ç”Ÿæˆé•¿åº¦: {gen_len}]")
    print("-" * 60)

    # 1. æ— ç¼“å­˜æ¨¡å¼ (Baseline)
    # æ³¨æ„ï¼šå¦‚æœæ˜¾å­˜ä¸å¤Ÿï¼Œè¯·è°ƒå° gen_lenï¼Œå› ä¸º O(N^2) çš„æ˜¾å­˜å¢é•¿éå¸¸å¿«
    try:
        t_no, ms_no, alloc_no, res_no = run_stress_test(model, src_ids, use_cache=False, max_tokens=gen_len)
        print(f"ã€æ— ç¼“å­˜å…¨é‡æ¨¡å¼ã€‘:")
        print(f"  > æ€»è€—æ—¶: {t_no:.2f}s | æ¯ Token å‡æ‘Š: {ms_no:.2f}ms")
        print(f"  > å³°å€¼åˆ†é… (å«æƒé‡): {alloc_no + base_mem:.2f}MB")
        print(f"  > ç³»ç»Ÿé¢„ç•™ (æ¥è¿‘smi): {res_no:.2f}MB")
    except RuntimeError as e:
        print("âŒ æ— ç¼“å­˜æ¨¡å¼ OOM (æ˜¾å­˜æº¢å‡º)ï¼è¿™è¯æ˜äº†å…¨é‡è®¡ç®—å¯¹æ˜¾å­˜çš„å·¨å¤§å‹åŠ›ã€‚")

    print("-" * 60)

    # 2. KV Cache æ¨¡å¼
    t_ca, ms_ca, alloc_ca, res_ca = run_stress_test(model, src_ids, use_cache=True, max_tokens=gen_len)
    print(f"ã€KV Cache å¢é‡æ¨¡å¼ã€‘:")
    print(f"  > æ€»è€—æ—¶: {t_ca:.2f}s | æ¯ Token å‡æ‘Š: {ms_ca:.2f}ms")
    print(f"  > å³°å€¼åˆ†é… (å«æƒé‡): {alloc_ca + base_mem:.2f}MB")
    print(f"  > ç³»ç»Ÿé¢„ç•™ (æ¥è¿‘smi): {res_ca:.2f}MB")

    print("-" * 60)
    print(f"ğŸ”¥ åŠ é€Ÿæ¯”: {t_no / t_ca:.2f}x")