# 定义

首先说一下进程、线程、协程的定义：
- 进程是 **操作系统分配的基本单位** ，每个进程之间的内存空间互不干扰
- 线程是 **OS 调度的基本单位** ，一个进程可以包含多个线程，每个线程共享进程的内存和计算资源
- 协程是用户态的轻量级线程，运行时并非由 OS 调度

其实不太好理解，但可以这么想，线程和协程的关系类比为：开会 vs 打电话
- 线程是开会的时候每个人（线程）都在说话，os来控制让谁说，多线程是抢占式调度
- 协程是这样，你和很多人打电话，其中一个协程碰到了 await，然后 你 不管他，转而去找一下个人打电话，是你可以控制的
他俩的关系是，线程是由 os 来控制什么时候被打断，而协程是你用 await 来控制什么时候被打断，是用户友好的

下面是一段代码示例：
```python
import time
import asyncio
import threading

# 协程
async def work(name, duration):
	print(f"[{time.time():.1f}] 开始 {name}")
	await asyncio.sleep(duration) # ✅ 异步等待，让出控制权
	print(f"[{time.time():.1f}] 结束 {name}")


async def main():
	# 同时启动三个任务，并发执行！
	await asyncio.gather(
		work("A", 2),
		work("B", 2),
		work("C", 2)
	)
asyncio.run(main())

# 线程
def twork(name, duration):
	print(f"[{time.time():.1f}] 开始 {name}")
	time.sleep(duration) # 线程中用同步 sleep 是 OK 的
	print(f"[{time.time():.1f}] 结束 {name}")

  
def tmain():
	threads = []
	for name in ["A", "B", "C"]:
		t = threading.Thread(target=twork, args=(name, 2))
		threads.append(t)
		t.start() # 启动线程

# 等待所有线程完成
for t in threads:
	t.join()

tmain()
```

这里可以看到，await 是程序员自己手动阻塞的，是对程序员更为友好的，但是线程你发出去，是由 os 来控制那个线程执行的

同时我们观察到，如果把 sleep 换成一个比如 for 的计算任务，那么这里多协程实际上会退化为串行，同时，由于 GIL 锁的关系，多线程也并不能发挥全部的实力，这是我们代码：
```python
async def work(name, duration):
	print(f"[{time.time():.1f}] 开始 {name}")
	for i in range(1000000):
		j = i + 1
	print(f"[{time.time():.1f}] 结束 {name}")
```
我们把多协程的 work 改成了一个计算任务，多进程类似，这时候的输出如下：
```markdown
[1769584166.6] 开始 A
[1769584166.6] 结束 A
[1769584166.6] 开始 B
[1769584166.6] 结束 B
[1769584166.6] 开始 C
[1769584166.7] 结束 C
多线程测试
[1769584166.7] 开始 A
[1769584166.7] 开始 B
[1769584166.7] 开始 C
[1769584167.8] 结束 B
[1769584167.9] 结束 A
[1769584168.0] 结束 C
```

我们可以看到，首先多协程是串行的，其次，原本串行的任务大概是 0.1 单位时间一个任务，但是在多线程中，当所有任务分发下去从 66 到 68 才完成，这就偏离预期了，所以我们知道 async 异步其实是并发 IO，但是串行执行任务

总结一下：
> 协程在 CPU-bound 代码中是不能切换执行权，因此在单线程事件中是表现为串行的
> 协程只有在 **非阻塞 IO 且显示 await** 的时候才能并发调度
> 
> 对线程来说，执行 CPU-bound 代码，通常来看多线程比单线程要更慢，因为线程还有自己的上下文要切换
> 在阻塞 IO 的时候，GIL通常会打开

什么是阻塞 IO 呢？
阻塞 IO 是发出一次 IO 后，当前的执行流程会被挂起，直到 IO 完成后才能够继续执行
比如我们前面的 sleep 的示例，在这段 IO 的时间，进程 / 线程什么都不能干，所以会被阻塞，叫阻塞 IO，ok，那么如何判断是不是阻塞 IO 呢？他需要等外部给结果：
- 等磁盘、网络、时间、用户这些操作都是阻塞 IO

那么这个时候 GIL 锁会放开，线程能够并发执行，但是协程要手动指定 await

## GIL 锁

首先 GIL 锁是什么？GIL 是 Global Interpreter Lock 全局解释锁，用来保证同一个时刻只有一个线程执行 python 的字节码
- Global：整个解释器进程只有一把
- Interpreter：不是 os，不是语言规范，是 CPython 实现细节
- Lock：互斥锁
- 执行 Python 字节码：重点，不是“线程不能并发”

为什么要有 GIL 锁？核心是内存管理，因为 CPython 使用：
- 引用计数（refcount）
- 每次对象赋值 / 释放会改变计数

如果没有 GIL，那么对于下面这个操作：
```python
a = b
--------------
incref(b) & decref(old_a)
```

也就是说，如果多线程情况下，这两个操作要是原子的，所以我们在CPython的设计中要加上一个细粒度的锁，这会导致：
1. 所有 对象操作 都需要加细粒度锁
2. 性能比较差
3. 解释器实现复杂

对于多线程，实际上 GIL 会隔一段时间释放GIL锁然后由os来调度其他线程，所以对于阻塞 IO 来说，线程可能会快一些，注意 GIL 只锁 python 字节码，所以：
- C/C++ 不锁
- numpy、torch 不锁
- CUDA Kernel 不锁

举个例子
```python
import torch

def twork():
    x = torch.randn(5000, 5000)
    y = torch.randn(5000, 5000)
    z = x @ y
```

梳理一下完整过程：
- x、y创建的时候，python调用 C++ 进行分配内存、填充随机数、纯 python 计算/内存操作，这里是没有 IO 的
- x@y的时候，这个python线程调用torch，torch调用他自己的底层算子库，这时候，这个 python 线程进到 torch 算子了，所以他主动释放了 GIL，让其他线程能够执行 python 字节码
- 算子算完，一步一步退到 python 写到 Z 的过程，重新拿到 GIL 的控制权，返回 python

# python中的并发模型

## asyncio
首先看一下异步、协程这些代码，早期 python 的协程是可以通过 yield 来实现的
```python
def consumer():
    r = ''
    while True:
        n = yield r
        if not n:
            return
        print('[CONSUMER] Consuming %s...' % n)
        r = '200 OK'

def produce(c):
    c.send(None)
    n = 0
    while n < 5:
        n = n + 1
        print('[PRODUCER] Producing %s...' % n)
        r = c.send(n)
        print('[PRODUCER] Consumer return: %s' % r)
    c.close()

c = consumer()
produce(c)
```

由于函数带了 yield，那么他就是一个 generate function，在编译期的时候把这个函数改成了一个 ”状态机“，所以一开始 c = consumer 其实没有运行函数

然后说一下 send 函数的作用：send(x) 的作用是先恢复 generator的执行，然后把 x 作为上一个 yield 表达式的返回值

所以我们看一下流程：
- 创建消费者，但是啥都没干
- 创建生产者
- 生产者 send(None)，执行到消费者的 yield r，这时 r 是 ''，同时生产者也没有使用变量去接收这个，所以他把 '' 给扔了相当于
然后进入生产则会的循环：
- 首先 n = 1
- 这时候 send 1，把 1 当做上一次 yield 的返回值，消费者的 n 拿到1，然后执行generator 的继续进行，到下一个 yield
-  ... ... 重复这个过程

这样就完成了两个协程间的交替运行

### 关于yield

关于 yield 可能不是特别清楚，这里还有个小例子：
（yield 是先给值然后挂起）
```python
def fab(max): 
	n, a, b = 0, 0, 1 
	while n < max: 
		yield b # 使用 yield 
		# print b 
		a, b = b, a + b 
		n = n + 1 
for n in fab(5): 
	print n
```

我们一步一步来看，首先用了 yield，那么他是一个迭代器函数，所以一开始创建 fab(5) 会卡主，那么 for 的时候（idx 表示迭代次数，从0开始）：
- idx = 0 -> 运行到 yield b，n = b = 1，然后挂起
- idx = 1 -> 这时候拿到 yield b的值，此时 b = a + b = 1，然后挂起
- idx = 2 -> 这时候拿到 b 的值，此时 b = a + b = 2，然后挂起
- idx = 3 ...
这里 for 就是调用的next

## threading 


## multiprocessing


# torch 中的并行模型