# 定义

首先说一下进程、线程、协程的定义：
- 进程是 **操作系统分配的基本单位** ，每个进程之间的内存空间互不干扰
- 线程是 **OS 调度的基本单位** ，一个进程可以包含多个线程，每个线程共享进程的内存和计算资源
- 协程是用户态的轻量级线程，运行时并非由 OS 调度

其实不太好理解，但可以这么想，线程和协程的关系类比为：开会 vs 打电话
- 线程是开会的时候每个人（线程）都在说话，os来控制让谁说，多线程是抢占式调度
- 协程是这样，你和很多人打电话，其中一个协程碰到了 await，然后 你 不管他，转而去找一下个人打电话，是你可以控制的
他俩的关系是，线程是由 os 来控制什么时候被打断，而协程是你用 await 来控制什么时候被打断，是用户友好的

下面是一段代码示例：
```python
import time
import asyncio
import threading

# 协程
async def work(name, duration):
	print(f"[{time.time():.1f}] 开始 {name}")
	await asyncio.sleep(duration) # ✅ 异步等待，让出控制权
	print(f"[{time.time():.1f}] 结束 {name}")


async def main():
	# 同时启动三个任务，并发执行！
	await asyncio.gather(
		work("A", 2),
		work("B", 2),
		work("C", 2)
	)
asyncio.run(main())

# 线程
def twork(name, duration):
	print(f"[{time.time():.1f}] 开始 {name}")
	time.sleep(duration) # 线程中用同步 sleep 是 OK 的
	print(f"[{time.time():.1f}] 结束 {name}")

  
def tmain():
	threads = []
	for name in ["A", "B", "C"]:
		t = threading.Thread(target=twork, args=(name, 2))
		threads.append(t)
		t.start() # 启动线程

# 等待所有线程完成
for t in threads:
	t.join()

tmain()
```

这里可以看到，await 是程序员自己手动阻塞的，是对程序员更为友好的，但是线程你发出去，是由 os 来控制那个线程执行的

同时我们观察到，如果把 sleep 换成一个比如 for 的计算任务，那么这里多协程实际上会退化为串行，同时，由于 GIL 锁的关系，多线程也并不能发挥全部的实力，这是我们代码：
```python
async def work(name, duration):
	print(f"[{time.time():.1f}] 开始 {name}")
	for i in range(1000000):
		j = i + 1
	print(f"[{time.time():.1f}] 结束 {name}")
```
我们把多协程的 work 改成了一个计算任务，多进程类似，这时候的输出如下：
```markdown
[1769584166.6] 开始 A
[1769584166.6] 结束 A
[1769584166.6] 开始 B
[1769584166.6] 结束 B
[1769584166.6] 开始 C
[1769584166.7] 结束 C
多线程测试
[1769584166.7] 开始 A
[1769584166.7] 开始 B
[1769584166.7] 开始 C
[1769584167.8] 结束 B
[1769584167.9] 结束 A
[1769584168.0] 结束 C
```

我们可以看到，首先多协程是串行的，其次，原本串行的任务大概是 0.1 单位时间一个任务，但是在多线程中，当所有任务分发下去从 66 到 68 才完成，这就偏离预期了，所以我们知道 async 异步其实是并发 IO，但是串行执行任务

总结一下：
> 协程在 CPU-bound 代码中是不能切换执行权，因此在单线程事件中是表现为串行的
> 协程只有在 **非阻塞 IO 且显示 await** 的时候才能并发调度
> 
> 对线程来说，执行 CPU-bound 代码，通常来看多线程比单线程要更慢，因为线程还有自己的上下文要切换
> 在阻塞 IO 的时候，GIL通常会打开

什么是阻塞 IO 呢？
阻塞 IO 是发出一次 IO 后，当前的执行流程会被挂起，直到 IO 完成后才能够继续执行
比如我们前面的 sleep 的示例，在这段 IO 的时间，进程 / 线程什么都不能干，所以会被阻塞，叫阻塞 IO，ok，那么如何判断是不是阻塞 IO 呢？他需要等外部给结果：
- 等磁盘、网络、时间、用户这些操作都是阻塞 IO

那么这个时候 GIL 锁会放开，线程能够并发执行，但是协程要手动指定 await

## GIL 锁

首先 GIL 锁是什么？GIL 是 Global Interpreter Lock 全局解释锁，用来保证同一个时刻只有一个线程执行 python 的字节码
- Global：整个解释器进程只有一把
- Interpreter：不是 os，不是语言规范，是 CPython 实现细节
- Lock：互斥锁
- 执行 Python 字节码：重点，不是“线程不能并发”

为什么要有 GIL 锁？核心是内存管理，因为 CPython 使用：
- 引用计数（refcount）
- 每次对象赋值 / 释放会改变计数

如果没有 GIL，那么对于下面这个操作：
```python
a = b
--------------
incref(b) & decref(old_a)
```

也就是说，如果多线程情况下，这两个操作要是原子的，所以我们在CPython的设计中要加上一个细粒度的锁，这会导致：
1. 所有 对象操作 都需要加细粒度锁
2. 性能比较差
3. 解释器实现复杂

对于多线程，实际上 GIL 会隔一段时间释放GIL锁然后由os来调度其他线程，所以对于阻塞 IO 来说，线程可能会快一些，注意 GIL 只锁 python 字节码，所以：
- C/C++ 不锁
- numpy、torch 不锁
- CUDA Kernel 不锁

举个例子
```python
import torch

def twork():
    x = torch.randn(5000, 5000)
    y = torch.randn(5000, 5000)
    z = x @ y
```

梳理一下完整过程：
- x、y创建的时候，python调用 C++ 进行分配内存、填充随机数、纯 python 计算/内存操作，这里是没有 IO 的
- x@y的时候，这个python线程调用torch，torch调用他自己的底层算子库，这时候，这个 python 线程进到 torch 算子了，所以他主动释放了 GIL，让其他线程能够执行 python 字节码
- 算子算完，一步一步退到 python 写到 Z 的过程，重新拿到 GIL 的控制权，返回 python

# python中的并发模型

## asyncio
>参考文章：[(14 封私信 / 4 条消息) 一文读懂Python async/await异步编程 - 知乎](https://zhuanlan.zhihu.com/p/698683843)


首先看一下异步、协程这些代码，早期 python 的协程是可以通过 yield 来实现的
```python
def consumer():
    r = ''
    while True:
        n = yield r
        if not n:
            return
        print('[CONSUMER] Consuming %s...' % n)
        r = '200 OK'

def produce(c):
    c.send(None)
    n = 0
    while n < 5:
        n = n + 1
        print('[PRODUCER] Producing %s...' % n)
        r = c.send(n)
        print('[PRODUCER] Consumer return: %s' % r)
    c.close()

c = consumer()
produce(c)
```

由于函数带了 yield，那么他就是一个 generate function，在编译期的时候把这个函数改成了一个 ”状态机“，所以一开始 c = consumer 其实没有运行函数

然后说一下 send 函数的作用：send(x) 的作用是先恢复 generator的执行，然后把 x 作为上一个 yield 表达式的返回值

所以我们看一下流程：
- 创建消费者，但是啥都没干
- 创建生产者
- 生产者 send(None)，执行到消费者的 yield r，这时 r 是 ''，同时生产者也没有使用变量去接收这个，所以他把 '' 给扔了相当于
然后进入生产则会的循环：
- 首先 n = 1
- 这时候 send 1，把 1 当做上一次 yield 的返回值，消费者的 n 拿到1，然后执行generator 的继续进行，到下一个 yield
-  ... ... 重复这个过程

这样就完成了两个协程间的交替运行

### 关于yield

关于 yield 可能不是特别清楚，这里还有个小例子：
（yield 是先给值然后挂起）
```python
def fab(max): 
	n, a, b = 0, 0, 1 
	while n < max: 
		yield b # 使用 yield 
		# print b 
		a, b = b, a + b 
		n = n + 1 
for n in fab(5): 
	print n
```

我们一步一步来看，首先用了 yield，那么他是一个迭代器函数，所以一开始创建 fab(5) 会卡主，那么 for 的时候（idx 表示迭代次数，从0开始）：
- idx = 0 -> 运行到 yield b，n = b = 1，然后挂起
- idx = 1 -> 这时候拿到 yield b的值，此时 b = a + b = 1，然后挂起
- idx = 2 -> 这时候拿到 b 的值，此时 b = a + b = 2，然后挂起
- idx = 3 ...
这里 for 就是调用的next

### 优雅使用async
首先，被 async 修饰的函数会被解释为一个 coroutine 函数，当我们对一个 coroutine 函数使用 await 的时候，当前函数中断，解释器开始执行 coroutine 的代码，和普通函数调用是一样的

例如：
```python
import asyncio
import time

async def async_hello_world():
    now = time.time()
    await asyncio.sleep(1)
    print(time.time() - now) # 1.0013360977172852
    print("Hello, world!") # Hello, world!
    await asyncio.sleep(1)
    print(time.time() - now) # 2.0025689601898193

print(asyncio.sleep(1)) # <coroutine object sleep at 0x102f663b0>
coro = async_hello_world()
asyncio.run(coro)
```

async 的优势是可以实现异步的并发操作，其中真正的对象是 Task

#### Task
在 python 的异步编程中，当我们对一个 Task 进行 await 的时候，event loop开始调度当前可执行的全部任务，直到被 await 的 Task 结束

可以说，task 才是调度的基本单位，前面对一个 coro 函数的 run 过程，实际上有以下几个步骤：
- 创建一个 event loop
- 将传入的 coro 包装成一个 Task
- 提交 Task 给 event loop
- 启动事件循环直到 Task 完成
- 返回 coro 返回值并关闭事件循环

实际对 task 的操作比如 create task，那么他的步骤是这样的：
- 接受一个 coro 并将他封装成 task
- 将这个 task 添加到 event loop中
- event 在下一次循环中执行 task

run 比他多了一步启动 event loop，create task代码示例可以如下：
```python
loop = asyncio.get_running_loop() # 获取当前正在运行的事件循环 task = loop.create_task(coro) # 创建 Task 并调度
```


关于task的示例例如：
```python
import asyncio
import time

async def async_hello_world():
    now = time.time()
    await asyncio.sleep(1)
    print(time.time() - now)
    print("Hello, world!")
    await asyncio.sleep(1)
    print(time.time() - now)

async def main():
    task1 = asyncio.create_task(async_hello_world())
    task2 = asyncio.create_task(async_hello_world())
    task3 = asyncio.create_task(async_hello_world())
    await task1
    await task2
    await task3

now = time.time()
# run 3 async_hello_world() coroutine concurrently
asyncio.run(main())

print(f"Total time for running 3 coroutine: {time.time() - now}")
```

那么这段代码的逻辑就比较清晰了：
- 将 main 这个 coro 创建为 task0
- 创建 event loop 并将 task0 添加到 event loop
- loop.run_until_compplete(main_task)
	- 进入 task1 创建，放到 loop 队列
	- 进入 task2 创建，放到 loop 队列
	- 进入 task3 创建，放到 loop 队列
	- await task1，中断当前 task0，可以让出控制权等待 task1 执行
		- task1 执行到 sleep 让出控制权，注册一个 1s 的 timer
	- loop 调度 task2 执行
		- task2 执行到 sleep 让出控制权
	- loop 调度 task3 执行
		- task3 执行到 sleep 让出控制权
- loop空转，1s 过后 task1 状态变为 ready
	- loop 调度 task1
		- 执行到 sleep
	- 调度 task2 并执行到 sleep
	- 调度 task3 并执行到 sleep
-   ... ... ... ...

#### Future

Future 和 Task 是类似的概念，简单来说：
- Future 关注结果
- Task 关注执行

Future 是一个状态机，有 res 和 exception，可以被 await
Task 绑定一个 coro，loop 能够主动 step 这个 coro， Task 是 Future 的子类

Future 只关心三件事：
1. 状态
    - `PENDING`
    - `FINISHED`
    - `CANCELLED`
2. 结果 / 异常
3. 回调（`add_done_callback`）

它不负责执行任何代码。

回到 Task，那么Task是如何推进 coro 的执行的呢？其实 coro 内部使用了yield，所以可以通过 send、throw 来控制执行， Task 推进 coro 的简单逻辑可以写为：
```python
while not coro.done():
    try:
        awaited = coro.send(None)
    except StopIteration as e:
        task.set_result(e.value)
        break

    # awaited 是一个 Future / Task
    awaited.add_done_callback(task._wakeup)
    break  # 让出控制权
```

下面是 Task 和 Future 协作的逻辑：
```python
async def f():
    await asyncio.sleep(1)
    return 10

task = asyncio.create_task(f())
```
执行关系是：
```markdown
event loop
  ↓
Task
  ↓
coroutine f
  ↓
await Future(sleep)
```

- sleep 返回的是一个Future
- Task 发现 coro 在等一个 Future
- Task 给这个 Future 注册回调
- Future 完成，唤醒 Task
- Task继续执行 coro

所以，更准确的说法是：
- event loop中调度的是 Task
- 被管理、阻塞的是 Future，Future 的任务是完成后给唤醒 Task

Task 怎么和 Future 对应的呢？
```python 
await fut
```
这相当于
```python
fut.add_done_callback(task._wakeup)
```
于是形成了一条链：
Future 完成 -> Task 唤醒 -> Task 继续跑

## threading 
>参考文章：[(12 封私信) python 高并发解决方案有哪些？ - 知乎](https://www.zhihu.com/question/647572334/answer/1995518625507996163)

### 线程创建
前面也说了，线程是 CPU 调度的最小单位，创建步骤是：
- 创建函数
- 创建线程
- 启动线程
- 结束线程

基本语法如下：
```python
# 导入线程库 
import threading 
# 函数 
def my_func(参数1,..参数n)： 
	代码块 
# 创建线程 
t = threading.Thread(target=my_func, args=(参数1,..参数n)) 
t.start() # 启动线程 
t.join() # 等待结束
```

### 线程锁

加锁主要还是解决不同线程操作数据的问题
锁的基本操作如下（以RLock 为例，RLock支持多次申请锁和多次释放锁）：
```python
Rlock=threading.RLock()

Rlock.acquire()   # 申请锁
Rlock.acquire()   # 申请锁
Rlock.release()  # 释放锁
Rlock.release()  # 释放锁


lock=threading.Lock()  # 创建锁
lock.acquire()   # 申请锁

try:
 代码块
finally:
 lock.release()  # 释放锁
 
# with 用法
with lock:
 # 代码块
```

#### RLock & Lock

简单来说：
- Lock 是不可重入互斥锁
- RLock 是可重入互斥锁

Lock 只关心资源有没有被占用而不记录谁占用
RLock会维护 占用 ID 和 占用计数

那么为啥需要 RLock 呢？其实是一种防御性编程，下面是一个例子
```python
class Cache:
    def get(self, key):
        with self.lock:
            return self._get_impl(key)

    def _get_impl(self, key):
        with self.lock:
            ...
```

比如这里，如果 self.lock 是一个 Lock 的话，那么他在进入 \_get_impl 会被卡死，而直接在 \_get_impl 内部加锁会有几个问题：
-  get_impl 可能在外部被单独调用
- 找不到调用路径

所以这时候用 RLock 更合适，以下几个场景可能更适合 RLock：
- 方法之间可能互相调用
- 公共方法调用私有方法，且都要保护同一资源
- 递归函数
- 你无法保证“只在最外层加锁”

所以能用 Lock 还是尽量用 Lock，Lock更为轻量级，RLock 不能防止逻辑死锁的问题，只能解决重入的问题
```python
def f():
    with rlock:
        g()

def g():
    with rlock:
        f()
```

所以上述代码还是会死循环，但不是死锁

### 信号量

信号量是针对共享资源的访问控制
当线程访问资源的时候，信号量 - 1，反之信号量 + 1，当信号量为 0 时，线程必须等待其他线程释放资源，所以对于一个公共变量吧，我们可以通过设置信号量为 1 来实现另类的锁，代码示例如下：
```python
class Account:
    def __init__(self,balance):
        self.balance=balance
account = Account(1000)

Semaphore=threading.Semaphore(1)  # 设置信号量为1
def draw(accout, amount):
    with Semaphore:        # 信号量
        if accout.balance > amount:
            time.sleep(2)
            print(threading.current_thread().name, '取钱成功！')
            accout.balance -= amount
            print(threading.current_thread().name, '余额', accout.balance)

        else:
            print(threading.current_thread().name, '取钱失败，余额不足'
```

### 守护线程
简单来说，守护线程不会阻止 **python 进程** 的线程
守护线程不参与进程存活判定

这也就是说，python 进程什么时候会退出呢？所有非守护线程都结束，进程就会继续运行到结束，不会管守护线程

那么守护线程有什么用呢？一般来说有以下几个作用：
- 日志刷盘
- 心跳
- metric 采集
- cache 预热
- watchdog
这些操作都是不重要、可以终端切不需要收尾的操作

看一个例子：
```python
import threading
import time

def daemon_worker():
    print("守护线程开始\n")
    time.sleep(5)
    print("守护线程结束\n")  # 不会执行，因为主线程已退出

d = threading.Thread(target=daemon_worker)
d.daemon = True  # 设置为守护线程
d.start()

print("主线程结束\n")
```

运行结果为：
```
守护线程开始
主进程结束
```

守护线程不会和主进程结束冲突或阻塞，但注意几个问题：
1. 守护线程必须在启动前设置 daemon，然后再 start
2. 守护线程依然收到 GIL 约束

### 线程池
字面意义理解，线程池就是维护线程的管理器，
![[ThreadPool.png]]

使用 python 自带的线程池有两种用法：
- map
- submit

#### map
map 的运行结果和入参顺序是对应一致的，如下所示：
```python
from concurrent.futures import ThreadPoolExecutor

with ThreadPoolExecutor(线程数量) as pool:  # 使用线程池
    result =pool.map(函数,参数)
    for i in result:
        print(i)
```

比如一个简单的爬虫代码：
```python
from concurrent.futures import ThreadPoolExecutor
import requests

urls = [f'https://www.cnblogs.com/#p{page}' for page in range(1, 50)]
def get_parse(url):
    response = requests.get(url)
    return response.url

with ThreadPoolExecutor(50) as pool:
    result =pool.map(get_parse,urls)
    for i in result:
        print(i)
```

结果如下：
```text
https://www.cnblogs.com/#1 
https://www.cnblogs.com/#2 
https://www.cnblogs.com/#3 
```

是和入参 urls 一一对应的

#### submit
那么 submit 则是顺序不定的
```python
with ThreadPoolExecutor(线程数量) as pool:
    futures=[pool.submit(函数,i) for i in 参数列表]

    for future in futures:     # 顺序输出
        print(future.result())  
    for future in as_completed(futures):  # 乱序输出
        print(future.result())
```

示例如下：
```python
from concurrent.futures import ThreadPoolExecutor,as_completed
import requests

urls = [f'https://www.cnblogs.com/#p{page}' for page in range(1, 50)]
def get_parse(url):
    response = requests.get(url)
    return response.url

with ThreadPoolExecutor() as pool:
    futures=[pool.submit(get_parse,url) for url in urls]
 
 # 乱序输出
    for future in as_completed(futures):
        print(future.result())
```

那么最终的结果是：
```python
https://www.cnblogs.com/#1
https://www.cnblogs.com/#12 
https://www.cnblogs.com/#4 
```

是不定的

## multiprocessing
>参考文章：[(8 封私信) 一篇文章搞定Python多进程(全) - 知乎](https://zhuanlan.zhihu.com/p/64702600)

### 进程创建
首先具体的语法和 threading 库都差不多

先来看他的创建
```python
from multiprocessing import Process

def fun1(name):
    print('测试%s多进程' %name)

process_list = []
for i in range(5):  #开启5个子进程执行fun1函数
    p = Process(target=fun1,args=(i,)) # 这里传参需要是可迭代的
    p.start()
    process_list.append(p)

for i in process_list:
    p.join()

print('结束测试')

# 通过继承来实现多进程
class MyProcess(Process):
    def __init__(self, name):
        super(MyProcess, self).__init__()
        self.name = str(name)    # 父类 Process 会调用他的 name setter，强制要求 str

    def run(self):
        print(f"测试{self.name}多线程")

process_list = []

for i in range(5):
    p = MyProcess(i)
    p.start()
    process_list.append(p)

for p in process_list:
    p.join()

print("结束测试")
```

这里 Process 对象还有其他一些创建参数
```python
class Process(
    group: None = None,
    target: ((...) -> object) | None = None,
    name: str | None = None,
    args: Iterable[Any] = (),
    kwargs: Mapping[str, Any] = {},
    *,
    daemon: bool | None = None
)
```

- group：emmm，保留参数，现在还没有任何作用
- name：进程名
- daemon：和守护线程类似，这里制定为守护进程

### 进程的通信
#### Queue

```python
from multiprocessing import Process,Queue


def fun1(q,i):
    print('子进程%s 开始put数据' %i)
    q.put('我是%s 通过Queue通信' %i)

if __name__ == '__main__':
    q = Queue()

    process_list = []
    for i in range(3):
        p = Process(target=fun1,args=(q,i,))  #注意args里面要把q对象传给我们要执行的方法，这样子进程才能和主进程用Queue来通信
        p.start()
        process_list.append(p)

    for i in process_list:
        p.join()

    print('主进程获取Queue数据')
    print(q.get())
    print(q.get())
    print(q.get())
    print('结束测试')
```

可以用 mp 库自带的 Queue 来进行通信，通过 Queue.put 和 get 来进行信息的push和获取
（tips：
- queue.Queue 这里的队列是线程安全的，因为他实现了 thread 级别的 lock
- mp 这里是进程安全因为他内部实现了进程级别的锁，底层是 Pipe、共享内存等原语

#### 管道
示例代码：
```python
from multiprocessing import Process, Pipe
def fun1(conn):
    print('子进程发送消息：')
    conn.send('你好主进程')
    print('子进程接受消息：')
    print(conn.recv())
    conn.close()

if __name__ == '__main__':
    conn1, conn2 = Pipe() #关键点，pipe实例化生成一个双向管
    p = Process(target=fun1, args=(conn2,)) #conn2传给子进程
    p.start()
    print('主进程接受消息：')
    print(conn1.recv())
    print('主进程发送消息：')
    conn1.send("你好子进程")
    p.join()
    print('结束测试')
```

emmmm，但是 Pipe 只能点对点通信，如果我们想把一个管道同时共享给几个子进程，多个子进程同时向一个 Pipe send，行为未定义可能导致阻塞、丢数据、崩溃等情况

#### Managers
还有个问题，Pipe 和 Queue 实现了进程的通信，但是没有实现数据的共享，或者可以说， Pipe 和 Queue 只实现了内部维护的字符串的共享

```python
from multiprocessing import Process, Manager

def fun1(dic,lis,index):
    dic[index] = 'a'
    lis.append(index)    #[0,1,2,3,4,0,1,2,3,4,5,6,7,8,9]
    #print(l)

if __name__ == '__main__':
    with Manager() as manager:
        dic = manager.dict()#注意字典的声明方式，不能直接通过{}来定义
        l = manager.list(range(5))#[0,1,2,3,4]

        process_list = []
        for i in range(10):
            p = Process(target=fun1, args=(dic,l,i))
            p.start()
            process_list.append(p)

        for res in process_list:
            res.join()
        print(dic)
        print(l)
```

结果如下：
```text
{0: 'a', 1: 'a', 2: 'a', 4: 'a', 3: 'a', 6: 'a', 5: 'a', 8: 'a', 9: 'a', 7: 'a'}
[0, 1, 2, 3, 4, 0, 1, 2, 4, 3, 6, 5, 8, 9, 7]
```
主进程定义了一个字典和一个列表，在子进程中，可以添加和修改字典的内容，在列表中插入新的数据，实现进程间的数据共享，即可以共同修改同一份数据

Manager 本身是一个单独的服务进程 + 一堆代理对象
```text
主进程
  ├─ dic (DictProxy) ─┐
  ├─ l   (ListProxy)  ├── RPC ──► Manager 进程
子进程                 ┘
```

真正的 dic、list 都在 Manager 进程里，他返回的是一对代理对象，当然 Manager 也支持注册自定义类型：
```python
from multiprocessing.managers import BaseManager

class MyObj:
    def __init__(self):
        self.x = 0
    def inc(self):
        self.x += 1
        return self.x

class MyManager(BaseManager):
    pass

MyManager.register('MyObj', MyObj)

if __name__ == "__main__":
    mgr = MyManager()
    mgr.start()
    obj = mgr.MyObj()
    print(obj.inc())  # RPC 调用
```

实际上 Manager 的效率很低，因为实际过程是：
- 子进程 -> pickle
- socket / pipe
- Manager 执行
- 返回
这是 IPC + 序列化，而不是内存写入

#### 进程池

```python
from  multiprocessing import Process,Pool
import os, time, random

def fun1(name):
    print('Run task %s (%s)...' % (name, os.getpid()))
    start = time.time()
    time.sleep(random.random() * 3)
    end = time.time()
    print('Task %s runs %0.2f seconds.' % (name, (end - start)))

if __name__=='__main__':
    pool = Pool(5) #创建一个5个进程的进程池

    for i in range(10):
        pool.apply_async(func=fun1, args=(i,))

    pool.close()
    pool.join()
    print('结束测试')
```

和线程池类似，注意一下 要先 close，关闭线程池的添加操作，然后再 join
