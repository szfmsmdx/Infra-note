# Profile

对于一个写好的模型，首先我们要知道他在那些地方是有问题的，这里我们使用 `torch.profile` 这个工具

```python
def run_profile():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Running on {device}")

    tokenizer = BPE_Tokenizer.load("../Transformers/Tokenizer/tokenizer.pt")
    config = LlamaConfig(tokenizer)
    config.num_layers = 16 
    config.dim = 512
    config.num_heads = 16
    config.intermediate_size = 1024
    config.num_kv_heads = 4
    model = LlamaGPT(config).to(device).bfloat16() # 使用 FP16，更符合推理场景
    
    bsz = 4
    seq_len = 256
    input_ids = torch.randint(0, config.vocab_size, (bsz, seq_len)).to(device)

    print("Warming up...")
    with torch.no_grad():
        for _ in range(5):
            model(input_ids)

    print("Profiling...")
    with profile(
        activities=[
            ProfilerActivity.CPU, 
            ProfilerActivity.CUDA
        ], 
        record_shapes=True,
        profile_memory=True  # 建议开启，可以看到算子的内存分配情况
    ) as prof:
        with record_function("model_inference"):
            with torch.no_grad():
                model(input_ids)

    print(prof.key_averages().table(sort_by="self_cuda_time_total", row_limit=15))

    prof.export_chrome_trace("trace.json")
    print("Trace saved to trace.json. Open chrome://tracing to view.")
```

一个简单实例，详细的 profile 操作可以参考：[144_推理时延优化：Profiling与瓶颈分析 - 使用PyTorch Profiler诊断推理延迟，优化矩阵运算的独特瓶颈-阿里云开发者社区](https://developer.aliyun.com/article/1684085)

首先，我们要给每个需要被检测的操作打上 profile 标签，例如：

```python
class LlamaLayer(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.attention = LlamaAttention(config)
        self.mlp = LlamaMLP(config.dim, config.intermediate_size)
        self.attention_norm = RMSNorm(config.dim)
        self.ffn_norm = RMSNorm(config.dim)

    def forward(self, x, freqs_cis, mask=None, kv_cache=None, start_pos=0):
        # h = x + self.attention(self.attention_norm(x), freqs_cis, mask, kv_cache, start_pos)
        # out = h + self.mlp(self.ffn_norm(h))
        with record_function("LlamaLayer_Total"):			# <-打上标签
            with record_function("Attention_Block"):
                h = x + self.attention(self.attention_norm(x), freqs_cis, mask, kv_cache, start_pos)
            with record_function("MLP_Block"):
                out = h + self.mlp(self.ffn_norm(h))
        return out
```

然后运行profile脚本，结果如下：

![[profile.png]]

其中 aten 是底层C++库（A Tensor Library），我们看打上标签的，比如 `RMS Norm` ，他的一些信息为：

- self cpu%：这个大算子自己占用的 cpu 时间占用整个 profile 的百分比
    - 值很高意味着 python 逻辑开销大，在进行费张量计算的操作（循环、逻辑判断等）
- self cpu：大算子自己占用的 cpu 时间（不包括下面的子算子）
- cpu total：大算子总的 cpu 时间
    - 衡量一个大模块在 cpu 调度层面的贡献
- cpu time avg：CPU total / # of Calls
    - 判断模型是否正确以及模块的平均贡献

cuda同理

- CPU Mem / Self CPU Mem：算子申请的 CPU 内存（通常是内存条上的空间）
    - 在推理阶段，如果这里有值，通常是数据在进行 `CPU <-> GPU` 的搬运（Host-to-Device）
- CUDA Mem / Self CUDA Mem：算子申请的显存（GPU 内存）
    - 正值表示算子申请了新的空间
    - 负值表示算子释放了额外的空间
    - 如果频繁正值说明操作中创建了 `new_tensor` 可能会加剧显存碎片并降低速度

总的来说，我们应该关注：

-  `CUDA total`：锁定哪个模块（比如 `RMSNorm`）最慢。
-  `# of Calls`：确定它是高频小算子还是低频大算子。
-  `Self CUDA`：当你写好 Triton 算子后，对比它和原来 `aten` 算子组合的 Self 时间。
-  `CUDA Mem`：检查你的优化是否成功减少了临时内存的申请。

在我们这里，我们看到，这里 RMS_Norm 显然不正常：

- 一方面，他的 cuda/cpu time avg 比矩阵乘还要高
- 另一方面，他的显存申请也比较高

所以我们首先考虑优化 RMS Norm这个算子

## RMS Norm

### 分析

```python
def forward(self, x):
        # norm_x = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        # return (norm_x.float()).type_as(x) * self.weight
        with record_function("OP: RMSNorm"):
            norm_x = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
            return (norm_x.float()).type_as(x) * self.weight
```

我们先看这里的`forward`写法，为什么会慢？

- 这里 norm_x 实际进行了
    - pow
    - mean
    - add
    - rsqrt
    - mul

五步操作，每个操作是一个小 kernel，那么就是大算子的调用次数 * 5，每个小 kernel 需要额外进行 CPU、GPU的通信开销，以及后续的 float 、type_as等操作，都是这个 rms norm算子慢的原因之一

### CUDA 优化


